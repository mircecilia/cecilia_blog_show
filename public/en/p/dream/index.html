<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ token top_p 1 2 3 4 5 6 7 8 9 10 11 12 def top_p_logits(logits, top_p=None): sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) sorted_indices_to_remove = cumulative_probs &gt; top_p sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 mask = torch.zeros_like(logits, dtype=torch.bool, device=logits.device) mask = mask.scatter_(-1, sorted_indices, sorted_indices_to_remove) logits = logits.masked_fill(mask, torch.finfo(logits.dtype).min) return logits Êú¨ÂáΩÊï∞ÂÖàÂ∞ÜÂæóÂà∞ÁöÑ logits ËøõË°åÈÄíÂáèÊéíÂ∫èÂπ∂Ëé∑ÂèñÁ¥¢ÂºïÔºåÁÑ∂ÂêéËÆ°ÁÆóÁ¥ØÂä†ÂíåÔºåË∂ÖËøá top_p Ê¶ÇÁéáÁöÑ logits Â∞ÜË¢´Ê†áËÆ∞‰∏∫ TrueÔºåÂêåÊó∂ÈÄöËøáÂ∞ÜÊï¥‰∏™ÂàóË°®Âè≥ÁßªÂπ∂Âú®Á¨¨‰∏Ä‰ΩçÂ°´ÂÖÖ 0 Êù•Âº∫Âà∂‰øùÁïôÊ¶ÇÁéáÊúÄÂ§ßÁöÑÂÄôÈÄâ tokenÔºåÂàõÂª∫‰∏Ä‰∏™ÂÖ®‰∏∫ Flase ÁöÑ mask ÂàóË°®ÔºåÂ∞Ü‰∏äËø∞‰øÆÊîπÂêéÁöÑÁ¥¢ÂºïÊò†Â∞ÑÂõûÂéªÔºåÊúÄÂêéÂ∞Ü mask ÂàóË°®‰∏≠Ê†áËÆ∞‰∏∫ True ÁöÑ‰ΩçÁΩÆÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÔºå‰ΩøÂÖ∂‰∏ç‰ºöË¢´ÈááÊ†∑\ngenerate example\n1 2 3 4 5 6 7 8 9 10 ÂÄôÈÄâ token -&gt; [A, B, C, D] logits -&gt; [2.0, 0.5, 1.0, -0.5] top_p -&gt; 0.8 softmax : probs -&gt; [0.57, 0.21, 0.14, 0.08] Á¥ØËÆ°Âíå -&gt; [0.57, 0.78, 0.92, 1.00] sorted_indices_to_remove -&gt; [False, False, True, True] -&gt; [False, False, False, True] mask -&gt; [False, False, False, True] logits -&gt; [2.0, 1.0, 0.5, -1e38] top_k 1 2 3 4 5 def top_k_logits(logits, top_k=None): top_k = min(top_k, logits.size(-1)) indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None] logits = logits.masked_fill(indices_to_remove, torch.finfo(logits.dtype).min) return logits Êú¨ÂáΩÊï∞ÂÖàËøõË°åÂÆâÂÖ®ÊÄßÊ£ÄÊü•ÔºåÈò≤Ê≠¢ top_k ÁöÑÂ§ßÂ∞èÂ§ß‰∫éËØçË°®Â§ßÂ∞èÔºåÁÑ∂ÂêéÂ∞ÜÂâç k Â§ßÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÈò≤Ê≠¢ÂÖ∂Ë¢´ÈááÊ†∑\n">
<title>Dream</title>

<link rel='canonical' href='https://mircecilia.netlify.app/en/p/dream/'>

<link rel="stylesheet" href="/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css"><meta property='og:title' content="Dream">
<meta property='og:description' content="Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ token top_p 1 2 3 4 5 6 7 8 9 10 11 12 def top_p_logits(logits, top_p=None): sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) sorted_indices_to_remove = cumulative_probs &gt; top_p sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 mask = torch.zeros_like(logits, dtype=torch.bool, device=logits.device) mask = mask.scatter_(-1, sorted_indices, sorted_indices_to_remove) logits = logits.masked_fill(mask, torch.finfo(logits.dtype).min) return logits Êú¨ÂáΩÊï∞ÂÖàÂ∞ÜÂæóÂà∞ÁöÑ logits ËøõË°åÈÄíÂáèÊéíÂ∫èÂπ∂Ëé∑ÂèñÁ¥¢ÂºïÔºåÁÑ∂ÂêéËÆ°ÁÆóÁ¥ØÂä†ÂíåÔºåË∂ÖËøá top_p Ê¶ÇÁéáÁöÑ logits Â∞ÜË¢´Ê†áËÆ∞‰∏∫ TrueÔºåÂêåÊó∂ÈÄöËøáÂ∞ÜÊï¥‰∏™ÂàóË°®Âè≥ÁßªÂπ∂Âú®Á¨¨‰∏Ä‰ΩçÂ°´ÂÖÖ 0 Êù•Âº∫Âà∂‰øùÁïôÊ¶ÇÁéáÊúÄÂ§ßÁöÑÂÄôÈÄâ tokenÔºåÂàõÂª∫‰∏Ä‰∏™ÂÖ®‰∏∫ Flase ÁöÑ mask ÂàóË°®ÔºåÂ∞Ü‰∏äËø∞‰øÆÊîπÂêéÁöÑÁ¥¢ÂºïÊò†Â∞ÑÂõûÂéªÔºåÊúÄÂêéÂ∞Ü mask ÂàóË°®‰∏≠Ê†áËÆ∞‰∏∫ True ÁöÑ‰ΩçÁΩÆÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÔºå‰ΩøÂÖ∂‰∏ç‰ºöË¢´ÈááÊ†∑\ngenerate example\n1 2 3 4 5 6 7 8 9 10 ÂÄôÈÄâ token -&gt; [A, B, C, D] logits -&gt; [2.0, 0.5, 1.0, -0.5] top_p -&gt; 0.8 softmax : probs -&gt; [0.57, 0.21, 0.14, 0.08] Á¥ØËÆ°Âíå -&gt; [0.57, 0.78, 0.92, 1.00] sorted_indices_to_remove -&gt; [False, False, True, True] -&gt; [False, False, False, True] mask -&gt; [False, False, False, True] logits -&gt; [2.0, 1.0, 0.5, -1e38] top_k 1 2 3 4 5 def top_k_logits(logits, top_k=None): top_k = min(top_k, logits.size(-1)) indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None] logits = logits.masked_fill(indices_to_remove, torch.finfo(logits.dtype).min) return logits Êú¨ÂáΩÊï∞ÂÖàËøõË°åÂÆâÂÖ®ÊÄßÊ£ÄÊü•ÔºåÈò≤Ê≠¢ top_k ÁöÑÂ§ßÂ∞èÂ§ß‰∫éËØçË°®Â§ßÂ∞èÔºåÁÑ∂ÂêéÂ∞ÜÂâç k Â§ßÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÈò≤Ê≠¢ÂÖ∂Ë¢´ÈááÊ†∑\n">
<meta property='og:url' content='https://mircecilia.netlify.app/en/p/dream/'>
<meta property='og:site_name' content='Cecilia'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-08-29T11:18:33&#43;08:00'/><meta property='article:modified_time' content='2025-08-29T11:18:33&#43;08:00'/>
<meta name="twitter:title" content="Dream">
<meta name="twitter:description" content="Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ token top_p 1 2 3 4 5 6 7 8 9 10 11 12 def top_p_logits(logits, top_p=None): sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) sorted_indices_to_remove = cumulative_probs &gt; top_p sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 mask = torch.zeros_like(logits, dtype=torch.bool, device=logits.device) mask = mask.scatter_(-1, sorted_indices, sorted_indices_to_remove) logits = logits.masked_fill(mask, torch.finfo(logits.dtype).min) return logits Êú¨ÂáΩÊï∞ÂÖàÂ∞ÜÂæóÂà∞ÁöÑ logits ËøõË°åÈÄíÂáèÊéíÂ∫èÂπ∂Ëé∑ÂèñÁ¥¢ÂºïÔºåÁÑ∂ÂêéËÆ°ÁÆóÁ¥ØÂä†ÂíåÔºåË∂ÖËøá top_p Ê¶ÇÁéáÁöÑ logits Â∞ÜË¢´Ê†áËÆ∞‰∏∫ TrueÔºåÂêåÊó∂ÈÄöËøáÂ∞ÜÊï¥‰∏™ÂàóË°®Âè≥ÁßªÂπ∂Âú®Á¨¨‰∏Ä‰ΩçÂ°´ÂÖÖ 0 Êù•Âº∫Âà∂‰øùÁïôÊ¶ÇÁéáÊúÄÂ§ßÁöÑÂÄôÈÄâ tokenÔºåÂàõÂª∫‰∏Ä‰∏™ÂÖ®‰∏∫ Flase ÁöÑ mask ÂàóË°®ÔºåÂ∞Ü‰∏äËø∞‰øÆÊîπÂêéÁöÑÁ¥¢ÂºïÊò†Â∞ÑÂõûÂéªÔºåÊúÄÂêéÂ∞Ü mask ÂàóË°®‰∏≠Ê†áËÆ∞‰∏∫ True ÁöÑ‰ΩçÁΩÆÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÔºå‰ΩøÂÖ∂‰∏ç‰ºöË¢´ÈááÊ†∑\ngenerate example\n1 2 3 4 5 6 7 8 9 10 ÂÄôÈÄâ token -&gt; [A, B, C, D] logits -&gt; [2.0, 0.5, 1.0, -0.5] top_p -&gt; 0.8 softmax : probs -&gt; [0.57, 0.21, 0.14, 0.08] Á¥ØËÆ°Âíå -&gt; [0.57, 0.78, 0.92, 1.00] sorted_indices_to_remove -&gt; [False, False, True, True] -&gt; [False, False, False, True] mask -&gt; [False, False, False, True] logits -&gt; [2.0, 1.0, 0.5, -1e38] top_k 1 2 3 4 5 def top_k_logits(logits, top_k=None): top_k = min(top_k, logits.size(-1)) indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None] logits = logits.masked_fill(indices_to_remove, torch.finfo(logits.dtype).min) return logits Êú¨ÂáΩÊï∞ÂÖàËøõË°åÂÆâÂÖ®ÊÄßÊ£ÄÊü•ÔºåÈò≤Ê≠¢ top_k ÁöÑÂ§ßÂ∞èÂ§ß‰∫éËØçË°®Â§ßÂ∞èÔºåÁÑ∂ÂêéÂ∞ÜÂâç k Â§ßÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÈò≤Ê≠¢ÂÖ∂Ë¢´ÈááÊ†∑\n">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/en/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_9932ff953b61ae7c.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">üê¶‚Äçüî•</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/en">Cecilia</a></h1>
            <h2 class="site-description">ÂñßÈóπÊï£ÂéªÊó∂ÔºåÊÑø‰Ω†‰∏çÂÜçÂ≠§Áã¨</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/mircecilia/cecilia_blog_show'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        <li class="menu-bottom-section">
            <ol class="menu">
                    
                        <li id="i18n-switch">  
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                            <select name="language" title="language" onchange="window.location.href = this.selectedOptions[0].value">
                                
                                    <option value="https://mircecilia.netlify.app/en/" selected>ÁßëÁ†îÂ∞èËÆ∞</option>
                                
                                    <option value="https://mircecilia.netlify.app/" >ÁÆóÊ≥ïÊùÇÁ¨î</option>
                                
                            </select>
                        </li>
                    
                

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ-token">Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ token</a>
      <ol>
        <li><a href="#top_p">top_p</a></li>
        <li><a href="#top_k">top_k</a></li>
      </ol>
    </li>
    <li><a href="#‰ªéÂÄôÈÄâ-token-‰∏≠ÈááÊ†∑">‰ªéÂÄôÈÄâ token ‰∏≠ÈááÊ†∑</a></li>
    <li><a href="#Á¶ªÊï£Êâ©Êï£Ëß£Á†Å">Á¶ªÊï£Êâ©Êï£Ëß£Á†Å</a></li>
    <li><a href="#rnsnorm">RNSNorm</a></li>
    <li><a href="#rope-È¢ëÁéáË°®ËÆ°ÁÆó">ROPE È¢ëÁéáË°®ËÆ°ÁÆó</a></li>
    <li><a href="#rope">ROPE</a></li>
    <li><a href="#mlp">MLP</a></li>
    <li><a href="#kv-Â§¥Êâ©Â±ï">K„ÄÅV Â§¥Êâ©Â±ï</a></li>
    <li><a href="#dreamattention">DreamAttention</a></li>
    <li><a href="#sdpa">SDPA</a></li>
    <li><a href="#decoderlayer">DecoderLayer</a></li>
    <li><a href="#transformer-ÂÆûÁé∞ÁªÜËäÇ">Transformer ÂÆûÁé∞ÁªÜËäÇ</a></li>
    <li><a href="#lm_head">lm_head</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/en/categories/pytorch/" >
                Pytorch
            </a>
        
            <a href="/en/categories/dllm/" >
                Dllm
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/p/dream/">Dream</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-08-29</time>
            </div>
        

        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ-token">Âü∫‰∫éÊ¶ÇÁéáÈòàÂÄº‰øùÁïôÂÄôÈÄâ token
</h2><h3 id="top_p">top_p
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">top_p_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices_to_remove</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">logits</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÂáΩÊï∞ÂÖàÂ∞ÜÂæóÂà∞ÁöÑ logits ËøõË°åÈÄíÂáèÊéíÂ∫èÂπ∂Ëé∑ÂèñÁ¥¢ÂºïÔºåÁÑ∂ÂêéËÆ°ÁÆóÁ¥ØÂä†ÂíåÔºåË∂ÖËøá top_p Ê¶ÇÁéáÁöÑ logits Â∞ÜË¢´Ê†áËÆ∞‰∏∫ TrueÔºåÂêåÊó∂ÈÄöËøáÂ∞ÜÊï¥‰∏™ÂàóË°®Âè≥ÁßªÂπ∂Âú®Á¨¨‰∏Ä‰ΩçÂ°´ÂÖÖ 0 Êù•Âº∫Âà∂‰øùÁïôÊ¶ÇÁéáÊúÄÂ§ßÁöÑÂÄôÈÄâ tokenÔºåÂàõÂª∫‰∏Ä‰∏™ÂÖ®‰∏∫ Flase ÁöÑ mask ÂàóË°®ÔºåÂ∞Ü‰∏äËø∞‰øÆÊîπÂêéÁöÑÁ¥¢ÂºïÊò†Â∞ÑÂõûÂéªÔºåÊúÄÂêéÂ∞Ü mask ÂàóË°®‰∏≠Ê†áËÆ∞‰∏∫ True ÁöÑ‰ΩçÁΩÆÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÔºå‰ΩøÂÖ∂‰∏ç‰ºöË¢´ÈááÊ†∑</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ÂÄôÈÄâ</span> <span class="n">token</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">top_p</span> <span class="o">-&gt;</span> <span class="mf">0.8</span>
</span></span><span class="line"><span class="cl"><span class="n">softmax</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">probs</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.21</span><span class="p">,</span> <span class="mf">0.14</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Á¥ØËÆ°Âíå</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.78</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">sorted_indices_to_remove</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">-&gt;</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">mask</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e38</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h3 id="top_k">top_k
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">top_k_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  
</span></span><span class="line"><span class="cl">    <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">logits</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÂáΩÊï∞ÂÖàËøõË°åÂÆâÂÖ®ÊÄßÊ£ÄÊü•ÔºåÈò≤Ê≠¢ top_k ÁöÑÂ§ßÂ∞èÂ§ß‰∫éËØçË°®Â§ßÂ∞èÔºåÁÑ∂ÂêéÂ∞ÜÂâç k Â§ßÁöÑ logits ËÆæÁΩÆ‰∏∫ÊûÅÂ∞èÂÄºÈò≤Ê≠¢ÂÖ∂Ë¢´ÈááÊ†∑</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">top_k</span> <span class="o">-&gt;</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">indices_to_remove</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e38</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e38</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="‰ªéÂÄôÈÄâ-token-‰∏≠ÈááÊ†∑">‰ªéÂÄôÈÄâ token ‰∏≠ÈááÊ†∑
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample_tokens</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">margin_confidence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">neg_entropy</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">top_p_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">top_k_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span> <span class="o">=</span> <span class="n">dists</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x0</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">confidence</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">margin_confidence</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">sorted_probs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">top1_probs</span> <span class="o">=</span> <span class="n">sorted_probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">        <span class="n">top2_probs</span> <span class="o">=</span> <span class="n">sorted_probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">        <span class="n">confidence</span> <span class="o">=</span> <span class="n">top1_probs</span> <span class="o">-</span> <span class="n">top2_probs</span> 
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">neg_entropy</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-10</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">confidence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probs</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">confidence</span><span class="p">,</span> <span class="n">x0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•ÂáΩÊï∞Ê†πÊçÆÊ∏©Â∫¶Â§ßÂ∞èÊéßÂà∂ logits ÂàÜÂ∏ÉÁöÑÂ∞ñÈîêÁ®ãÂ∫¶ÔºåÁÑ∂ÂêéÂ∫îÁî® top_k / top_p Á≠ñÁï•ËøáÊª§ÂèØËÉΩÊÄß‰ΩéÁöÑ tokenÔºå‰πãÂêé‰ΩøÁî® softmax Â∞Ü logits ÂΩí‰∏ÄÂåñ‰∏∫Ê¶ÇÁéáÂàÜÂ∏ÉÔºåËã•Ê∏©Â∫¶Â§ß‰∫éÈõ∂ÔºåÂàô‰ΩøÁî®Â§öÈ°πÂàÜÂ∏ÉÈááÊ†∑ÔºåÊåâÁÖßÂêÑ logits Ê¶ÇÁéáÈÄâÂá∫Ëß£Á†Å tokenÔºåÂê¶ÂàôÁõ¥Êé•ÈÄâÂèñÊ¶ÇÁéáÊúÄÂ§ßÁöÑ token</p>
<p>Ëã•‰ΩøÁî® margin_confidenceÔºåÂàôÁΩÆ‰ø°Â∫¶ÂÆö‰πâ‰∏∫Á¨¨‰∏ÄÂêçÊ¶ÇÁéáÂáèÂéªÁ¨¨‰∫åÂêçÊ¶ÇÁéáÔºåËã•‰ΩøÁî® neg_entropyÔºåÂàôÁΩÆ‰ø°Â∫¶ÂÆö‰πâ‰∏∫Ë¥üÁÜµÔºåÂç≥ÂàÜÂ∏ÉË∂äÂ∞ñÈîêÁΩÆ‰ø°Â∫¶Ë∂äÈ´ò</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ÈááÁî®</span> <span class="n">top_k</span> <span class="o">/</span> <span class="n">top_p</span> <span class="n">ËøáÊª§Âêé</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">temperture</span> <span class="o">-&gt;</span> <span class="mi">0</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">softmax</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.62</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">ÈÄâÊã©</span> <span class="n">token</span> <span class="o">-&gt;</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl"><span class="n">confidence</span> <span class="o">-&gt;</span> <span class="mf">0.62</span> <span class="p">;</span> <span class="n">x0</span> <span class="o">-&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">temperture</span> <span class="o">-&gt;</span> <span class="mi">2</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">softmax</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.48</span><span class="p">,</span> <span class="mf">0.29</span><span class="p">,</span><span class="mf">0.23</span><span class="p">]</span> <span class="c1"># ÂàÜÂ∏ÉÊõ¥Âπ≥Áºì</span>
</span></span><span class="line"><span class="cl"><span class="n">ÈÄâÊã©</span> <span class="n">token</span> <span class="o">-&gt;</span> <span class="n">A</span> <span class="o">/</span> <span class="n">B</span> <span class="o">/</span> <span class="n">C</span> <span class="p">(</span><span class="n">ÊåâÁÖßÊ¶ÇÁéáÈöèÊú∫ÊäΩÊ†∑</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">confidence</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">margin_confidence</span> <span class="o">-&gt;</span> <span class="mf">0.19</span> <span class="p">;</span> <span class="n">x0</span> <span class="o">-&gt;</span> <span class="mi">0</span> <span class="o">/</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">neg_entropy</span> <span class="o">-&gt;</span> <span class="o">-</span><span class="mf">1.05</span> <span class="p">;</span> <span class="n">x0</span> <span class="o">-&gt;</span> <span class="mi">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="Á¶ªÊï£Êâ©Êï£Ëß£Á†Å">Á¶ªÊï£Êâ©Êï£Ëß£Á†Å
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_sample</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">generation_config</span><span class="p">:</span> <span class="n">DreamGenerationConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">generation_tokens_hook_func</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">generation_logits_hook_func</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">DreamModelOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_history</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_history</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">mask_token_id</span>
</span></span><span class="line"><span class="cl">        <span class="n">steps</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">steps</span>
</span></span><span class="line"><span class="cl">        <span class="n">eps</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eps</span>
</span></span><span class="line"><span class="cl">        <span class="n">alg</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">alg</span>
</span></span><span class="line"><span class="cl">        <span class="n">alg_temp</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">alg_temp</span>
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">temperature</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_p</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_p</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_k</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">histories</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_history</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•ÊÆµË¥üË¥£ÂàùÂßãÂåñÔºå‰ªé generation_config ‰∏≠ËØªÂèñÁõ∏Â∫îÁöÑÂèÇÊï∞ÔºåÂπ∂ÂàõÂª∫‰∏Ä‰∏™Á©∫ÂàóË°®‰øùÂ≠ò‰∏≠Èó¥ÁöÑÈááÊ†∑ËøáÁ®ã</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">-</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">value</span><span class="o">=</span><span class="n">mask_token_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">attention_mask</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tok_idx</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">tok_idx</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">attention_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tok_idx</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="s2">&#34;full&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•ÊÆµÂ∞ÜËæìÂÖ•Â∫èÂàóÁöÑÂè≥‰æßÂÖ®ÈÉ®‰ΩøÁî® mask_token_id Â°´ÂÖÖÔºåÂêåÊó∂Âà§Êñ≠ÊòØÂê¶ËøõË°åÊ≥®ÊÑèÂäõÈÅÆËîΩÔºåËã•ËøõË°åÊ≥®ÊÑèÂäõÈÅÆËîΩÔºåÂàôÊâ©Â±ï‰º†ÂÖ•ÁöÑ attention_mask Ëá≥ max_length ÔºåÁÑ∂ÂêéÁîüÊàêÁ¨¶Âêà transformer Êû∂ÊûÑÁöÑÂõõÁª¥Âº†ÈáèÔºå‰ΩøÊ≥®ÊÑèÂäõÊú∫Âà∂Âè™Âú®ÈùûÈÅÆËîΩ token ‰∏äËøõË°åÔºåÂêåÊó∂ÁîüÊàêÊúâÊïà token ‰ΩçÁΩÆÁõ∏Â∫îÁöÑÁ¥¢Âºï</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">attention_mask</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">max_length</span> <span class="o">-&gt;</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl"><span class="n">attention_mask</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">tok_idx</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">-&gt;</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">attention_mask</span> <span class="o">-&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">generation_tokens_hook_func</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">mask_token_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">tok_idx</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">logits</span><span class="p">[:,:</span><span class="mi">1</span><span class="p">],</span> <span class="n">logits</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">generation_logits_hook_func</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mask_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">mask_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">alg</span> <span class="o">==</span> <span class="s1">&#39;origin&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_transfer</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">s</span> <span class="o">/</span> <span class="n">t</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">steps</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">mask_index</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask_token_id</span>
</span></span><span class="line"><span class="cl">        <span class="n">transfer_index_t_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p_transfer</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index_t_s</span><span class="p">]</span><span class="o">=</span> <span class="n">sample_tokens</span><span class="p">(</span><span class="n">mask_logits</span><span class="p">[</span><span class="n">transfer_index_t_s</span><span class="p">],</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">[</span><span class="n">mask_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">alg</span> <span class="o">==</span> <span class="s1">&#39;maskgit_plus&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">sample_tokens</span><span class="p">(</span><span class="n">mask_logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">alg</span> <span class="o">==</span> <span class="s1">&#39;topk_margin&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">sample_tokens</span><span class="p">(</span><span class="n">mask_logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">margin_confidence</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">alg</span> <span class="o">==</span> <span class="s1">&#39;entropy&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span><span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">sample_tokens</span><span class="p">(</span><span class="n">mask_logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">neg_entropy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Unknown alg: </span><span class="si">{</span><span class="n">alg</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÂÖàÁîüÊàê‰ªé 1 ÈÄíÂáèËá≥ eps ÁöÑÊó∂Èó¥Â∫èÂàóÔºåÁÑ∂ÂêéË∞ÉÁî®ÂâçÂêë‰º†Êí≠ÂáΩÊï∞ÁîüÊàê logits Âπ∂Êï¥‰ΩìËøõË°åÂè≥ÁßªÔºå‰ª•‰æøÂØπÈΩêÈ¢ÑÊµã token ‰ΩçÁΩÆÔºåËÆ°ÁÆó‰∫§ÂèâÁÜµ loss</p>
<blockquote>
<p><strong>origin</strong> : ÂÖàËÆ°ÁÆóÊõøÊç¢ÊØî‰æã p_transferÔºå‰πãÂêéÂú®ÊØè‰∏™‰ΩçÁΩÆ‰∏äÈöèÊú∫Ëé∑ÂæóÊ¶ÇÁéáÔºå‰Ωé‰∫éËØ•Ê¶ÇÁéáÁöÑ‰ΩçÁΩÆË¢´Â°´ÂÖÖ‰∏∫ TrueÔºå‰πãÂêéË∞ÉÁî® sample_tokensÔºåÂú®ÊØè‰∏™ True ‰ΩçÁΩÆ‰∏äÔºåÊ†πÊçÆÊâÄÈÄâÁÆóÊ≥ïÁîüÊàêË¢´ÈááÊ†∑ÁöÑ tokenÔºåÊúÄÂêéÂØπ x ËøõË°åÊõ¥Êñ∞</p></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">num_mask_token</span> <span class="o">=</span> <span class="n">mask_index</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask_index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">number_transfer_tokens</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_mask_token</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span> <span class="o">/</span> <span class="n">t</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">steps</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_mask_token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">full_confidence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">full_confidence</span><span class="p">[</span><span class="n">mask_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">confidence</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">number_transfer_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">alg_temp</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">alg_temp</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">full_confidence</span><span class="p">,</span> <span class="n">number_transfer_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_confidence</span> <span class="o">=</span> <span class="n">full_confidence</span> <span class="o">/</span> <span class="n">alg_temp</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_confidence</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">full_confidence</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">full_confidence</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">number_transfer_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask_token_id</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_</span><span class="p">[</span><span class="n">mask_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">row_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">transfer_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">[</span><span class="n">row_indices</span><span class="p">,</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_</span><span class="p">[</span><span class="n">row_indices</span><span class="p">,</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•ÊÆµÂáΩÊï∞ËÆ°ÁÆóÊú¨Ê≠•Ëß£Á†ÅÁöÑ token Êï∞ÔºåÂêåÊó∂Â∞ÜÁΩÆ‰ø°Â∫¶Â°´Âú®Ëß£Á†Å‰ΩçÁΩÆ‰∏äÔºåÂπ∂Â∞ÜÂÖ∂‰Ωô‰ΩçÁΩÆÁöÑÁΩÆ‰ø°Â∫¶ËÆæÁΩÆ‰∏∫ -infÔºåËã•‰ΩøÁî® softmax Ëß£Á†ÅÔºåÂàôÂ∞ÜÁΩÆ‰ø°Â∫¶ËøõË°åÂΩí‰∏ÄÂåñÔºå‰ΩøÁî®ÂÖ∂Ê¶ÇÁéáËøõË°åÈöèÊú∫ÊäΩÊ†∑Ëß£Á†ÅÔºåÂê¶ÂàôÈÄâÊã©ÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑ‰ΩçÁΩÆËøõË°åËß£Á†ÅÔºåÊúÄÂêé‰ΩøÁî®ÈááÊ†∑ÁªìÊûú x0 Ë¶ÜÂÜô x</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">generation_tokens_hook_func</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">histories</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">histories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">DreamModelOutput</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">sequences</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">history</span><span class="o">=</span><span class="n">histories</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•ÊÆµÂáΩÊï∞ÈÄöËøá histories ‰øùÂ≠òÂπ∂Êü•ÁúãËß£Á†ÅËøáÁ®ãÔºåËøîÂõûÊúÄÁªàÁöÑÁîüÊàêÂ∫èÂàó</p>
<h2 id="rnsnorm">RNSNorm
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamRMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">variance</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÂÖà‰øùÂ≠òËæìÂÖ• dtypeÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ float32 ‰ª•‰øùËØÅÊï∞ÂÄºÁ®≥ÂÆöÔºåÈöèÂêéÂØπ embedding ÂêéÁöÑÂêëÈáèÊ±ÇÂùáÊñπÊ†πÔºå‰øùËØÅÂêëÈáè‰∏çÊîπÂèòÊñπÂêëËÄåÂè™ÊîπÂèòÈïøÂ∫¶Ôºå‰∏çÊçüÂÆ≥Âü∫Êú¨ËØ≠‰πâ</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">forward</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.1826</span><span class="p">,</span> <span class="mf">0.3651</span><span class="p">,</span> <span class="mf">0.5477</span><span class="p">,</span> <span class="mf">0.7303</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="rope-È¢ëÁéáË°®ËÆ°ÁÆó">ROPE È¢ëÁéáË°®ËÆ°ÁÆó
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s2">&#34;dynamic&#34;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">rope_type</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_frequency_update</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">inv_freq_expanded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">position_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_ids_expanded</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">device_type</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
</span></span><span class="line"><span class="cl">    <span class="n">device_type</span> <span class="o">=</span> <span class="n">device_type</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">device_type</span> <span class="o">!=</span> <span class="s2">&#34;mps&#34;</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">freqs</span> <span class="o">=</span> <span class="p">(</span><span class="n">inv_freq_expanded</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">@</span> <span class="n">position_ids_expanded</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">cos</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">sin</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_scaling</span>
</span></span><span class="line"><span class="cl">    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_scaling</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">cos</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">sin</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ROPE ÊâÄÂØπÂ∫îÁöÑÂáΩÊï∞ÈÉ®ÂàÜÂÖàËøõË°åÂàùÂßãÂåñÔºåÂêåÊó∂Ê∑ªÂä†ÈáçÁΩÆÂèÇÊï∞ÔºåÂä®ÊÄÅÊõ¥Êñ∞‰∏§‰∏™ËæÖÂä©ÂáΩÊï∞ÔºåÂú®Ê≠§‰∏çÂÅöÊëòÂΩïÔºåÊ≠§ÊÆµÂáΩÊï∞ÊòØ ROPE ÁöÑÈ¢ëÁéáË°®ÂÆûÁé∞ÔºåÂÖàÂ§çÂà∂ self.inv_freq Ëá≥ÊØè‰ªΩ batchÔºåÁÑ∂ÂêéÂØπ position_ids ËøõË°åÊâ©Âº†‰∏éËΩ¨ÁΩÆÔºå‰ΩøÂÖ∂ÂèØ‰ª•Âêå inv_freq_expanded ÂÅöÁü©Èòµ‰πòÊ≥ïÔºå‰∫åËÄÖÁõ∏‰πòÂêéËΩ¨ÁΩÆÔºåÊãºÊé•ÔºåÂç≥ÂèØÂæóÂà∞ÊØè‰∏™Ê†∑Êú¨Âú®ÂØπÂ∫îÁöÑ‰ΩçÁΩÆÔºåÁª¥Â∫¶ÈúÄË¶ÅÊóãËΩ¨ÁöÑËßíÂ∫¶</p>
<blockquote>
<ul>
<li>
<p>Âú®Ê≠§ËØ¥Êòé‰∏∫‰ªÄ‰πàË¶ÅÂØπ emb Ë°®ËøõË°åÂ§çÂà∂ÔºåÊãºÊé• Ôºö</p>
<p>Áî±‰∫é ROPE ÁöÑÂéüÁêÜÊòØÊØè‰∏§‰∏™Áª¥Â∫¶ÂàÜ‰∏∫‰∏ÄÁªÑÔºåÂõ†Ê≠§Âú®ÂÖàÂâçËÆ°ÁÆó freq Ë°®Êó∂ÔºåÂÆûÈôÖ‰∏äÊòØÂú®ËÆ°ÁÆóÊØè‰∏§‰∏™Áª¥Â∫¶ÂàÜÁªÑÂêéÂØπÂ∫îÁöÑËßíÂ∫¶ÔºåÊïÖÂæóÂà∞ÁöÑÈ¢ëÁéáË°®ÈúÄË¶ÅÂ§çÂà∂ÂêéÊãºÊé•ÊâçËÉΩÂØπÂ∫îÂéüÂÖàÁöÑ hidden_layer Áª¥Â∫¶</p>
</li>
<li>
<p>ÂØπ‰∫é‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÊüê‰∏™ token ÂêëÈáèÔºåÈ¢ëÁéáË°®ËÆ°ÁÆóÊñπÊ≥ïÂ¶Ç‰∏ãÔºö</p>
<p>inv_freq_expanded -&gt; [[1.0], [0.1], [0.01]]</p>
<p>position_ids_expanded -&gt; [3]</p>
<p>freqs -&gt; [3.0, 0.3, 0.03]</p>
<p>-&gt; [3.0, 0.3, 0.03, 3.0, 0.3, 0.03]</p>
</li>
<li>
<p>ÈÇ£‰πà‰∏∫‰ªÄ‰πà‰∏çÊòØ [3.0, 3.0, 0.3, 0.3, 0.03, 0.03] Âë¢Ôºü</p>
<p>‰∫ãÂÆû‰∏äÔºåROPE ÁöÑÂàÜÁªÑÊóãËΩ¨Âπ∂‰∏çÊòØÊåâÁõ∏ÈÇªÁª¥Â∫¶ËøõË°åÁöÑÔºåÂØπ Q Âº†ÈáèÊù•ËØ¥ÔºåÊòØÊåâ (Q1, Q4) , (Q2, Q5) , (Q3, Q6) ËøõË°åÂàÜÂâ≤ÁöÑÔºåÂéüÂõ†ÊòØ ROPE Â∞ÜÊï¥‰∏™ÂêëÈáèËßÜ‰∏∫‰∏Ä‰∏™ÂàÜÂâçÂêé‰∏§ÂçäÁöÑÂ§çÊï∞ (x1 + x2 i) ËøõË°åÊóãËΩ¨ÔºåÂõ†Ê≠§ÂâçÂçäÊÆµÁ¨¨‰∏Ä‰∏™ÁöÑÁª¥Â∫¶ÂØπÂ∫îÂêéÂçäÊÆµÁöÑÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶ÔºåÂÖ∑‰ΩìÂéüÁêÜÂú® rotate_half ‰∏≠ËØ¶ÁªÜÈòêÊòé</p>
</li>
</ul></blockquote>
<h2 id="rope">ROPE
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">unsqueeze_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">unsqueeze_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">unsqueeze_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">q_embed</span><span class="p">,</span> <span class="n">k_embed</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÊòØ ROPE ÁöÑÊ†∏ÂøÉÂÆûÁé∞Ôºårotate_half ÂÖàÂ∞Ü embedding ÂêéÁöÑÂêëÈáè (hidden state) ÊãÜÊàêÂâçÂêé‰∏§ÂçäÔºåÂØπÂêéÂçäÂèñÂèçÂêéÊãºÊé•ÔºåÂ∞ÜÊï¥‰∏™ÂêëÈáè (x1, x2) ËßÜ‰ΩúÂ§çÊï∞ (x1 + x2 i) ËøõË°å‰πùÂçÅÂ∫¶ÊóãËΩ¨</p>
<p>apply_rotary_pos_emb Â∞Ü cos ‰∏é sin ËøõË°åÂçáÁª¥ÂπøÊí≠Ôºå‰πãÂêéË∞ÉÁî®  rotate_halfÂ∞Ü Q„ÄÅK ÁöÑÂêÑÁª¥Â∫¶ÊàêÂØπËøõË°åÊóãËΩ¨ÔºåÊúÄÁªàËøîÂõûÊ∑ªÂä†‰ΩçÁΩÆ‰ø°ÊÅØÂêéÁöÑ Q„ÄÅK ÂêëÈáè</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">rotate_half</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">-&gt;</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">apply_rotary_pos_emb</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">Q</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">cos</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">sin</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.866</span><span class="p">,</span> <span class="mf">0.866</span><span class="p">,</span> <span class="mf">0.866</span><span class="p">,</span> <span class="mf">0.866</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">rotate_half</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Q</span> <span class="o">*</span> <span class="n">cos</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">rotate_half</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.598</span><span class="p">,</span><span class="o">-</span><span class="mf">3.464</span><span class="p">,</span><span class="mf">0.866</span><span class="p">,</span><span class="mf">1.732</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Q_embed</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.098</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.464</span><span class="p">,</span> <span class="mf">2.366</span><span class="p">,</span> <span class="mf">3.732</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="mlp">MLP
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÊòØ MLP ÁöÑÊ†∏ÂøÉÂÆûÁé∞ÔºåÂåÖÂê´ up_proj , down_proj , gate_proj ‰∏â‰∏™Â±Ç‰ª•Âèä SiLu ÊøÄÊ¥ªÂáΩÊï∞ÔºåËøô‰∏â‰∏™Â±Ç‰∏≠ÂêÑÊúâ‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊùÉÈáçÁü©Èòµ</p>
<p>ÂØπÊØè‰∏™ token Êù•ËØ¥ÔºåÂÖ∂ÂêëÈáèÂÖà‰∏é gate_proj ‰∏≠ÁöÑÁü©ÈòµÂÅöÁü©Èòµ‰πòÊ≥ïÔºå‰ΩøÂÖ∂ÊãìÂ±ïËá≥Êõ¥È´òÁª¥Â∫¶ÔºåÂπ∂ÈÄöËøá SiLu ÂáΩÊï∞ËøõË°åÊøÄÊ¥ªÔºåÂæóÂà∞ gate Áü©ÈòµÔºåÁÑ∂ÂêéÂÜç‰∏é up_proj ‰∏≠ÁöÑÁü©ÈòµÂÅöÁü©Èòµ‰πòÊ≥ïÔºåÂæóÂà∞ up Áü©ÈòµÔºåÂ∞Ü up Áü©Èòµ‰∏é gate Áü©ÈòµÂÅöÂÖÉÁ¥†‰πòÊ≥ïÔºåÂæóÂà∞ h Áü©ÈòµÔºåÊúÄÂêéÂ∞ÜÊâÄÂæó h Áü©Èòµ‰∏é down_proj ‰∏≠ÁöÑÁü©ÈòµÂÅöÁü©Èòµ‰πòÊ≥ïÔºåÂ∞ÜÁª¥Â∫¶ÂéãÁº©Ëá≥ÂéüÈöêËóèÂ±ÇÁª¥Â∫¶Ôºå‰∏∫ token Â¢ûÂä†Êõ¥Â§öÊ†∑ÁöÑ‰ø°ÊÅØ</p>
<blockquote>
<p>ÊãìÂ±ïÂêéÁöÑÁª¥Â∫¶Â§ßÂ∞èÂêç‰∏∫ intermediate_sizeÔºåÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ôºå‰∏ÄËà¨ÊòØ hidden_layer ÁöÑÂõõÂÄçÔºåÂØπ GPT3 Êù•ËØ¥ÔºåÊï¥‰∏™Ê®°ÂûãÁ∫¶‰∏âÂàÜ‰πã‰∫åÁöÑÂèÇÊï∞ÈáèÈÉΩ‰øùÂ≠òÂú® MLP Â±ÇÁöÑÂêÑÁßçÊùÉÈáçÁü©Èòµ‰∏≠ÔºåËøô‰∫õÂèÇÊï∞ËÆ∞ÂΩï‰∫ÜÊ®°ÂûãÂ≠¶‰π†Âà∞ÁöÑÂêÑÁßçÁü•ËØÜ</p></blockquote>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">2.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">W_gate</span> <span class="p">:</span>  
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">W_up</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">W_down</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">g</span> <span class="o">=</span> <span class="n">W_gate</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">3.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">gate</span> <span class="o">=</span> <span class="n">SiLu</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">2.8578</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">0.1888</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">0.0335</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">up</span> <span class="o">=</span> <span class="n">W_up</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="mf">3.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">h</span> <span class="o">=</span> <span class="n">gate</span> <span class="o">*</span> <span class="n">up</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">0.8573</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">0.0944</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.0402</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="mf">0.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">W_down</span> <span class="o">@</span> <span class="n">h</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">0.1122</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span><span class="mf">0.3603</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="kv-Â§¥Êâ©Â±ï">K„ÄÅV Â§¥Êâ©Â±ï
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">repeat_kv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">n_rep</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">n_rep</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÂÆûÁé∞‰∫ÜÂØπ K„ÄÅV Â§¥ÁöÑÊâ©Â±ïÔºå‰ΩøÂÖ∂‰∏é Q Â§¥Êï∞ÈáèÁõ∏Á≠âÔºå‰ª•Á¨¶Âêà‰º†Áªü transformer Êû∂ÊûÑÔºåÂÖ∑‰ΩìÊìç‰ΩúÊòØÂú® num_key_value_heads ÂêéÊ∑ªÂä†‰∏Ä‰∏™Áª¥Â∫¶Âπ∂Â§çÂà∂ n_rep Ê¨°Ôºåreshape ÂêéÂæóÂà∞ÊúÄÁªàÁöÑÂ§¥Êï∞</p>
<blockquote>
<p>‰º†Áªü transformer ‰∏≠ K„ÄÅQ„ÄÅV ÁöÑÂ§¥Êï∞ÊòØÁõ∏ÂêåÁöÑÔºå‰ª•‰æøËøõË°åÁÇπÁßØÔºåÁî®‰∫éËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞Ôºå‰ΩÜÂú® LLaMA ËÆæËÆ°ÁöÑ GQA Êû∂ÊûÑ‰∏≠Ôºå‰∏∫‰∫ÜÂáèÂ∞ë KV_cache Â∏¶Êù•ÁöÑÂ≠òÂÇ®‰ª•ÂèäËÆ°ÁÆóÂéãÂäõÔºåÂÖ∂ÂáèÂ∞ë‰∫Ü K„ÄÅV ÁöÑÂ§¥Êï∞ÔºåËÄå Dream Áî±‰∫éÊó†Ê≥ïÂ§çÁî® KV_cache ÔºåÊ≤øÁî®‰∫Ü‰∏é LLaMA Áõ∏ÂêåÁöÑËÆæËÆ°Ôºå‰ª•ÁºìËß£ÊØèÊ¨°ÈáçÊñ∞ÈááÊ†∑ÂêéËÆ°ÁÆó K„ÄÅV Â∏¶Êù•ÁöÑÁÆóÂäõÂéãÂäõ</p></blockquote>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span>  <span class="c1"># Â§¥0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">9.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">11.0</span><span class="p">,</span> <span class="mf">12.0</span><span class="p">]]</span> <span class="c1"># Â§¥1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">repeat_kv</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span>  <span class="c1"># Â§¥0(copy_1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span>  <span class="c1"># Â§¥0(copy_2)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">9.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">11.0</span><span class="p">,</span> <span class="mf">12.0</span><span class="p">]]</span> <span class="c1"># Â§¥1(copy1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">9.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">11.0</span><span class="p">,</span> <span class="mf">12.0</span><span class="p">]]</span> <span class="c1"># Â§¥1(copy_2)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="dreamattention">DreamAttention
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DreamConfig</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">layer_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="sa">f</span><span class="s2">&#34;Instantiating </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> without passing `layer_idx` is not recommended and will &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;when creating this class.&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rope_theta</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="sa">f</span><span class="s2">&#34;hidden_size must be divisible by num_heads (got `hidden_size`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="sa">f</span><span class="s2">&#34; and `num_heads`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">).&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">DreamRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÂÖàÂØπÈöêËóèÂ±ÇÁª¥Â∫¶ÔºåÂ§öÂ§¥Êï∞ÈáèÔºåÈúÄË¶ÅÂ§çÂà∂ÁöÑ K„ÄÅV Êï∞ÈáèÔºåÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶ÔºåROPE È¢ëÁéáË°®Ôºådropout ÊØî‰æãËøõË°åÂàùÂßãÂåñÔºåÈªòËÆ§‰∏ç‰ΩøÁî®Âõ†ÊûúÊé©Á†ÅÔºå‰ΩøÊ®°ÂûãÂÖ∑ÊúâÂÖ®Â±ÄËßÜËßíÔºå‰πãÂêéÂàõÂª∫ q_projÔºåk_projÔºåv_projÔºåo_proj Âõõ‰∏™Â±ÇÔºå‰∏∫ÂêéÁª≠ËøõË°åÊ≥®ÊÑèÂäõÊú∫Âà∂ÂÅöÂáÜÂ§á</p>
<blockquote>
<ul>
<li>
<p>‰∫ãÂÆû‰∏äÔºåÁªèËøáÂ§öÂ§¥Ê≥®ÊÑèÂäõËÆ°ÁÆóÂêéÔºåÊãºÂõûÁöÑËæìÂá∫ÁöÑÁª¥Â∫¶‰∏éÂéüÈöêËóèÂ±ÇÁª¥Â∫¶ÊòØÁõ∏ÂêåÁöÑÔºåÈÇ£‰πà‰∏∫‰ªÄ‰πàË¶ÅÂÜçÁªèËøá o_proj Â±Ç‰∏ÄÊ¨°Âë¢Ôºü</p>
<p>ÂéüÂõ†ÊòØÔºöÊãºÂõûÁöÑÊñ∞ÂêëÈáèËôΩÁÑ∂Áª¥Â∫¶Áõ∏ÂêåÔºå‰ΩÜÊòØ‰∏çËÉΩËûçÂêàÂêÑ‰∏™Â§¥‰πãÈó¥ÁöÑ‰ø°ÊÅØÔºåËÄå o_proj Â±ÇÂèØ‰ª•ËÆ©ÂêÑ‰∏™Â§¥Ëé∑ÂèñÁöÑ‰ø°ÊÅØÂΩºÊ≠§‰∫§ËûçÔºåÂà©Áî®‰∏çÂêåÁöÑÁâπÂæÅÔºå‰ΩøÁîüÊàêÁöÑÂè•Â≠êÊõ¥Âä†ÁîüÂä®</p>
</li>
<li>
<p>ÂêåÊó∂Ôºåo_proj Â±Ç‰πüÊòØÂîØ‰∏Ä‰∏ç‰ΩøÁî® bias ÁöÑÂ±ÇÔºåÂõ†‰∏∫Ê≠§Â§ÑÂêëÈáèÁöÑÂπ≥ÁßªËÉΩÂäõ‰ºöÂú®ÂêéÁª≠ÊÆãÂ∑ÆÊ∑ªÂä†‰∏≠‰ΩìÁé∞ÔºåÊó†ÈúÄÂºïÂÖ• bias</p>
</li>
</ul></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Cache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># will become mandatory in v4.46</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">position_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;The attention layers in this model are transitioning from computing the RoPE embeddings internally &#34;</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed &#34;</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be &#34;</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;removed and `position_embeddings` will be mandatory.&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">position_embeddings</span>
</span></span><span class="line"><span class="cl">    <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;sin&#34;</span><span class="p">:</span> <span class="n">sin</span><span class="p">,</span> <span class="s2">&#34;cos&#34;</span><span class="p">:</span> <span class="n">cos</span><span class="p">,</span> <span class="s2">&#34;cache_position&#34;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">cache_kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">+</span> <span class="n">causal_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="sa">f</span><span class="s2">&#34;`attn_output` should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="sa">f</span><span class="s2">&#34; </span><span class="si">{</span><span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_attentions</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">past_key_value</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÊòØ DreamAttention ËÆ°ÁÆóÁöÑÊ†∏ÂøÉÂÆûÁé∞ÔºåÂÖàÂ∞Ü token ÂêëÈáèÁªèËøá‰∏âÊ¨°ÂèòÊç¢ÔºåÂæóÂà∞ K„ÄÅQ„ÄÅV ÂêëÈáèÂêéÂÜçÂ∞ÜÂÖ∂ÊãÜÊàêÂ§öÂ§¥Ôºå‰πãÂêé reshape ‰ª•‰æøËøõË°åÁü©ÈòµËÆ°ÁÆó</p>
<p>Â∫îÁî® ROPE ‰∏∫ Q„ÄÅK ÂêëÈáèÊ∑ªÂä†‰ΩçÁΩÆ‰ø°ÊÅØÔºåËã•ÂêØÁî® KV_cacheÔºåÂàôÂ∞ÜÊñ∞ token ÁöÑ K„ÄÅV ËøΩÂä†Ëá≥‰πãÂâçÁöÑ K„ÄÅV ÂêéÔºåËã•‰ΩøÁî® MQAÔºåÂàôÂ§çÂà∂Êâ©Â±ï K„ÄÅV Â§¥Êï∞Ôºå‰Ωø Q„ÄÅK„ÄÅV Â§¥Êï∞Áõ∏Á≠âÔºå‰πãÂêéËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞ÔºåÂêåÊó∂ÈÄâÊã©ÊòØÂê¶Ê∑ªÂä†Êé©Á†ÅÔºåÂπ∂‰øùËØÅÂΩ¢Áä∂ÂØπÈΩêËá≥ k_len</p>
<p>ÂæóÂà∞Ê≥®ÊÑèÂäõÂàÜÊï∞ÂêéÔºå‰ΩøÁî® softmax Â∞ÜÂÖ∂ËΩ¨‰∏∫Ê¶ÇÁéáÂàÜÂ∏ÉÔºåÂêåÊó∂‰ΩøÁî® dropout Èò≤Ê≠¢ËøáÊãüÂêàÔºåÂú®ÈÄöËøáÊ≥®ÊÑèÂäõÊùÉÈáçÂä†ÊùÉÊ±ÇÂíå Value ÂêéÔºåÊ†°È™åÂΩ¢Áä∂ÊòØÂê¶Ê≠£Á°ÆÔºåÂπ∂Â∞ÜÂ§öÂ§¥ÈáçÊñ∞ÊãºÊé•ÔºåÁªèËøá o_proj Â±ÇÂêéÔºåËøîÂõûÂä†ÊùÉËÆ°ÁÆóÊâÄÂæóÁöÑ V</p>
<h2 id="sdpa">SDPA
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamSdpaAttention</span><span class="p">(</span><span class="n">DreamAttention</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Cache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;DreamModel is using DreamSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=&#34;eager&#34;` when loading the model.&#39;</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">position_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;The attention layers in this model are transitioning from computing the RoPE embeddings internally &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;removed and `position_embeddings` will be mandatory.&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">position_embeddings</span>
</span></span><span class="line"><span class="cl">        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">cache_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;sin&#34;</span><span class="p">:</span> <span class="n">sin</span><span class="p">,</span> <span class="s2">&#34;cos&#34;</span><span class="p">:</span> <span class="n">cos</span><span class="p">,</span> <span class="s2">&#34;cache_position&#34;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">}</span>  
</span></span><span class="line"><span class="cl">            <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">cache_kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">query_states</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&#34;cuda&#34;</span> <span class="ow">and</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">query_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">key_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">value_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÊòØ DreamAttention ÁöÑ‰ºòÂåñÁâàÊú¨ÔºåÂîØ‰∏ÄÁöÑÊîπÂä®Âú®‰∫é‰ΩøÁî®‰∫Ü PyTorch Êèê‰æõÁöÑ scaled_dot_product_attention ‰ª£ÊõøÊâãÂÜô softmax Ê≥®ÊÑèÂäõÔºåËØ•ÂÅöÊ≥ïËôΩÁÑ∂‰∏çËÉΩËøîÂõûÊ≥®ÊÑèÂäõÊùÉÈáçÁü©ÈòµÔºå‰ΩÜÊòØ‰ºö‰ΩøÊÄßËÉΩÊèêÈ´ò</p>
<blockquote>
<p>scaled_dot_product_attention ÂÜÖÈÉ®‰ºöË∞ÉÁî® FlashAttentionÔºåÈÄöËøáÂàáÂùóÊäÄÊúØÂ∞ÜËÆ°ÁÆóÂàÜËß£‰∏∫Â∞èÂùóÔºåÂú® GPU ÁöÑ SRAM ‰∏äÂÆåÊàêÂ§ßÈÉ®ÂàÜËÆ°ÁÆó</p></blockquote>
<h2 id="decoderlayer">DecoderLayer
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DreamConfig</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">sliding_window</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">!=</span> <span class="s2">&#34;flash_attention_2&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="sa">f</span><span class="s2">&#34;Sliding Window Attention is enabled but not implemented for `</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span><span class="si">}</span><span class="s2">`; &#34;</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;unexpected results may be encountered.&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">DreamSdpaAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">DreamMLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">DreamRMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">DreamRMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÂÖàÂàùÂßãÂåñÂèÇÊï∞ÔºåÂÆö‰πâ MLP Â±Ç‰∏é‰∏§‰∏™ RMSNorm Â±ÇÔºå‰∏∫ÂêéÁª≠ËøõË°åËß£Á†ÅÂÅöÂáÜÂ§á</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  
</span></span><span class="line"><span class="cl">    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_value</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">position_embeddings</span><span class="o">=</span><span class="n">position_embeddings</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">present_key_value</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="n">outputs</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Êú¨ÊÆµÂáΩÊï∞ÊòØ decoder ÁöÑÊ†∏ÂøÉÂÆûÁé∞ÔºåÂÖàÂØπ hidden_states ÂÅö‰∏ÄÊ¨° RMSNormÔºåÁ°Æ‰øùÊ¢ØÂ∫¶Á®≥ÂÆöÔºåÂÜçË∞ÉÁî® DreamSdpaAttention ÂÆûÁé∞Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰πãÂêéËøõË°åÊÆãÂ∑ÆËøûÊé•ÔºåÂ∞ÜÂæóÂà∞ÁöÑËæìÂá∫ÈÄÅÂÖ• MLP Â±Ç‰∏≠</p>
<p>Âú® MLP Â±Ç‰∏≠ÔºåÂÖàÂØπ hidden_states ËøõË°å‰∏ÄÊ¨° NormÔºåÂÜçË∞ÉÁî® DreamMLP ËøõË°åËÆ°ÁÆóÔºåÊúÄÂêéËøõË°åÊÆãÂ∑ÆËøûÊé•ÔºåËæìÂá∫ hidden_states</p>
<blockquote>
<p><strong>flow chart</strong></p>
<p>|</p>
<p>|&mdash;&mdash;&mdash; Norm &mdash;&mdash;&mdash;&gt; Attention &mdash;&mdash;&mdash;&gt; + residual &mdash;&mdash;&mdash;&gt; x_attention</p>
<p>|</p>
<p>|&mdash;&mdash;&mdash; Norm &mdash;&mdash;&mdash;&gt; MLP &mdash;&mdash;&mdash;&gt; + residual &mdash;&mdash;&ndash;&gt; Output</p></blockquote>
<h2 id="transformer-ÂÆûÁé∞ÁªÜËäÇ">Transformer ÂÆûÁé∞ÁªÜËäÇ
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamBaseModel</span><span class="p">(</span><span class="n">DreamPreTrainedModel</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">BaseModelOutput</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">inputs_embeds</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">decoder_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">decoder_layer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">position_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">past_key_values</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">output_attentions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">use_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">cache_position</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">position_embeddings</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">past_key_value</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">position_embeddings</span><span class="o">=</span><span class="n">position_embeddings</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">BaseModelOutput</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attns</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•Á±ªÊòØÊï¥‰∏™ transformer È™®Âπ≤ÁöÑÊ†∏ÂøÉÂÆûÁé∞ÔºåÂú®Ê≠§Êàë‰ª¨Âè™ÊëòÂΩïÈÉ®ÂàÜÂÆûÁé∞ÁªÜËäÇÔºåforward ÊñπÊ≥ïÂÖàÂ∞Ü ROPE ÊèêÂâçËÆ°ÁÆóÂ•ΩÔºåÁÑ∂Âêé‰∏çÊñ≠Â†ÜÂè†Ê≥®ÊÑèÂäõËÆ°ÁÆóÂ±Ç‰∏é MLP Â±ÇÔºåÂ∞ÜÊØèÊ¨°ÂæóÂà∞ÁöÑ hidden_states ËøõË°å‰øùÂ≠òÂêéÔºåÂÜç‰º†Áªô‰∏ã‰∏ÄÂ±Ç decoder_layerÔºåÂÆûÁé∞Â§öÂ±ÇÂè†Âä†ÔºåÊúÄÁªàÂæóÂà∞ËæìÂá∫ hidden_states</p>
<h2 id="lm_head">lm_head
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DreamModel</span><span class="p">(</span><span class="n">DreamGenerationMixin</span><span class="p">,</span> <span class="n">DreamPreTrainedModel</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;lm_head.weight&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_logits_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">**</span><span class="n">loss_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">MaskedLMOutput</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="n">num_logits_to_keep</span><span class="p">:,</span> <span class="p">:])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="o">**</span><span class="n">loss_kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">MaskedLMOutput</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ËØ•Á±ªÁöÑ‰ΩúÁî®ÊòØËÆ°ÁÆóÊúÄÁªàËØçË°®ÁöÑ logits ÔºåÂú®ÂèñÂá∫ÊúÄÁªàÁöÑ hidden_states ÂêéÔºåÂ∞ÜÂÖ∂‰∏éËØçË°®ÁöÑÊùÉÈáçÁü©ÈòµÁõ∏‰πòÔºåÂæóÂà∞È¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØçÁöÑ logitsÔºåÂêåÊó∂ËÆ°ÁÆó‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÊúÄÁªàÂ∞Ü lossÔºålogitsÔºåhidden_statesÔºåÊ≥®ÊÑèÂäõÊùÉÈáçÁü©Èòµ‰∏ÄÂπ∂ËøîÂõû</p>
<blockquote>
<p><strong>generate example</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">lm_head</span><span class="o">.</span><span class="n">wight</span><span class="o">.</span><span class="n">data</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># vocab[0]</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># vocab[1]</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># vocab[2]</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># vocab[3]</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># vocab[4]</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>

</section>


    <footer class="article-footer">
    

    </footer>


    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/p/llada/">
        
        

        <div class="article-details">
            <h2 class="article-title">LLaDA</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2025 Cecilia
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
